{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8ca4efa",
   "metadata": {},
   "source": [
    "# 语言模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de7f0c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments, DataCollatorForLanguageModeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "123c1981",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.load_from_disk(\"datas/wiki_cn_filtered\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a823cc06",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"models/bloom-389\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "978c6ceb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['source', 'completion'],\n",
       "    num_rows: 10000\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fbc67ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_func(examples):\n",
    "    examples = [sentence + tokenizer.eos_token for sentence in examples[\"completion\"]]\n",
    "    return tokenizer(examples, max_length=384, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "652bb82a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f52ac457fda4016b6b2e104090783c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_data = dataset.map(process_func, batched=True, remove_columns=dataset.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed52ea92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask'],\n",
       "    num_rows: 10000\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f8456a6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [13110, 34800, 13535, 916, 33156, 10, 256, 576, 387, 479, 681, 5453, 10955, 915, 24124, 5317, 13110, 6573, 20757, 13535, 355, 5358, 1490, 583, 28056, 1407, 3855, 671, 6113, 189, 6732, 4302, 9488, 3434, 6900, 1322, 355, 37336, 9825, 4608, 13461, 1359, 5358, 355, 5317, 13110, 34800, 4433, 7189, 25722, 29747, 13110, 1498, 12047, 6347, 23563, 2139, 2066, 420, 29288, 25, 15, 7635, 39288, 355, 1484, 5835, 6272, 23, 15, 4180, 39288, 355, 5358, 4516, 11621, 23, 15, 10641, 4887, 1712, 420, 2450, 31163, 8085, 11621, 5358, 553, 9888, 2731, 21335, 5358, 553, 9876, 14011, 4434, 5358, 553, 21484, 4514, 17170, 25871, 8085, 5358, 553, 17489, 6945, 11097, 13535, 641, 33623, 1484, 5835, 1689, 2063, 5358, 569, 5835, 671, 16225, 3422, 189, 13, 23158, 27894, 33227, 1022, 11396, 3347, 1813, 1504, 6566, 1813, 355, 9155, 8633, 1504, 2063, 1813, 189, 13, 23158, 813, 7817, 5358, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "73355379",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"models/bloom-389\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "aba7ec09",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir=\"trained/model_for_causalLM\",\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=16,\n",
    "    logging_steps=30,\n",
    "    num_train_epochs=1,\n",
    "    fp16=True,\n",
    "    optim=\"adafactor\",\n",
    "    learning_rate=1e-5,\n",
    "    weight_decay=0.01\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1d4f3b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    args=args,\n",
    "    model=model,\n",
    "    train_dataset=tokenized_data,\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d7d62bc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='313' max='313' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [313/313 04:12, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>3.220300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>3.230300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>3.289800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>3.273900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>3.287400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>3.328700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>3.369700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>3.394500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>3.375800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>3.393900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=313, training_loss=3.322041033175045, metrics={'train_runtime': 253.629, 'train_samples_per_second': 39.428, 'train_steps_per_second': 1.234, 'total_flos': 6102526349377536.0, 'train_loss': 3.322041033175045, 'epoch': 1.0})"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5cb7cff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BloomForCausalLM, DataCollatorForLanguageModeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9d6f26ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "50af1111",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c233d591",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': '西安博物馆（ึชญญ머จฮษญำ）是中国西安的一座博物馆。该博物馆由中国西安市文物局投资、西安市文物博物馆管理，西安市博物馆协会管理。该博物馆位于西安市碑林区南大街10号。\\n历史\\n该博物馆始建于1969年，是西安市最早建立的文物博物馆，也是中国西安市博物馆协会的所在地。1996年，该博物馆被列为全国第一批重点文物保护单位。2004年，该博物馆被列为首批全国重点文物保护单位。2009年，该博物馆被列为首批全国重点文物保护单位。2010年，该博物馆被列为第一批全国重点文物保护单位。2013年'}]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe(\"西安博物馆\", max_new_tokens=128, do_sample=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae25015",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7a749b25",
   "metadata": {},
   "source": [
    "# 掩码语言模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8873584",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM, Trainer, TrainingArguments, DataCollatorForLanguageModeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99c4decc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.load_from_disk(\"datas/wiki_cn_filtered\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0fd7c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"models/macbert-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a57da45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_func(examples):\n",
    "    return tokenizer(examples[\"completion\"], max_length=384, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f0c9181f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8baea94cac24755ac5b08935bfee288",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_data = dataset.map(process_func, batched=True, remove_columns=dataset.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e07c84b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at models/macbert-base were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForMaskedLM.from_pretrained(\"models/macbert-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b587903c",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir=\"trained/model_for_maskedlLM\",\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=16,\n",
    "    logging_steps=30,\n",
    "    num_train_epochs=1,\n",
    "    fp16=True,\n",
    "    optim=\"adafactor\",\n",
    "    learning_rate=1e-5,\n",
    "    weight_decay=0.01\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "68a32436",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    args=args,\n",
    "    model=model,\n",
    "    train_dataset=tokenized_data,\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e4b96201",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='313' max='313' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [313/313 02:53, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.437700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.372300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>1.343900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>1.354700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.317700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>1.307000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>1.291900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>1.328700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>1.329900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.315400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=313, training_loss=1.3359024303789717, metrics={'train_runtime': 173.8249, 'train_samples_per_second': 57.529, 'train_steps_per_second': 1.801, 'total_flos': 1872398224800768.0, 'train_loss': 1.3359024303789717, 'epoch': 1.0})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "921e5504",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"fill-mask\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "78becbc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'score': 0.9988579750061035,\n",
       "   'token': 1920,\n",
       "   'token_str': '大',\n",
       "   'sequence': \"[CLS] 西 安 交 通 大 [MASK] 博 物 馆 （ xi ' an jiaotong university museum ） 是 一 座 位 于 西 安 交 通 大 学 的 博 物 馆 [SEP]\"},\n",
       "  {'score': 0.0008164722239598632,\n",
       "   'token': 2110,\n",
       "   'token_str': '学',\n",
       "   'sequence': \"[CLS] 西 安 交 通 学 [MASK] 博 物 馆 （ xi ' an jiaotong university museum ） 是 一 座 位 于 西 安 交 通 大 学 的 博 物 馆 [SEP]\"},\n",
       "  {'score': 9.377801325172186e-05,\n",
       "   'token': 2339,\n",
       "   'token_str': '工',\n",
       "   'sequence': \"[CLS] 西 安 交 通 工 [MASK] 博 物 馆 （ xi ' an jiaotong university museum ） 是 一 座 位 于 西 安 交 通 大 学 的 博 物 馆 [SEP]\"},\n",
       "  {'score': 6.345337169477716e-05,\n",
       "   'token': 4906,\n",
       "   'token_str': '科',\n",
       "   'sequence': \"[CLS] 西 安 交 通 科 [MASK] 博 物 馆 （ xi ' an jiaotong university museum ） 是 一 座 位 于 西 安 交 通 大 学 的 博 物 馆 [SEP]\"},\n",
       "  {'score': 1.5071497728058603e-05,\n",
       "   'token': 2825,\n",
       "   'token_str': '技',\n",
       "   'sequence': \"[CLS] 西 安 交 通 技 [MASK] 博 物 馆 （ xi ' an jiaotong university museum ） 是 一 座 位 于 西 安 交 通 大 学 的 博 物 馆 [SEP]\"}],\n",
       " [{'score': 0.9985925555229187,\n",
       "   'token': 2110,\n",
       "   'token_str': '学',\n",
       "   'sequence': \"[CLS] 西 安 交 通 [MASK] 学 博 物 馆 （ xi ' an jiaotong university museum ） 是 一 座 位 于 西 安 交 通 大 学 的 博 物 馆 [SEP]\"},\n",
       "  {'score': 0.0009468726930208504,\n",
       "   'token': 1920,\n",
       "   'token_str': '大',\n",
       "   'sequence': \"[CLS] 西 安 交 通 [MASK] 大 博 物 馆 （ xi ' an jiaotong university museum ） 是 一 座 位 于 西 安 交 通 大 学 的 博 物 馆 [SEP]\"},\n",
       "  {'score': 0.00025484731304459274,\n",
       "   'token': 7368,\n",
       "   'token_str': '院',\n",
       "   'sequence': \"[CLS] 西 安 交 通 [MASK] 院 博 物 馆 （ xi ' an jiaotong university museum ） 是 一 座 位 于 西 安 交 通 大 学 的 博 物 馆 [SEP]\"},\n",
       "  {'score': 8.145409810822457e-05,\n",
       "   'token': 3413,\n",
       "   'token_str': '校',\n",
       "   'sequence': \"[CLS] 西 安 交 通 [MASK] 校 博 物 馆 （ xi ' an jiaotong university museum ） 是 一 座 位 于 西 安 交 通 大 学 的 博 物 馆 [SEP]\"},\n",
       "  {'score': 1.5790597899467684e-05,\n",
       "   'token': 3318,\n",
       "   'token_str': '术',\n",
       "   'sequence': \"[CLS] 西 安 交 通 [MASK] 术 博 物 馆 （ xi ' an jiaotong university museum ） 是 一 座 位 于 西 安 交 通 大 学 的 博 物 馆 [SEP]\"}]]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe(\"西安交通[MASK][MASK]博物馆（Xi'an Jiaotong University Museum）是一座位于西安交通大学的博物馆\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3391b8d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
