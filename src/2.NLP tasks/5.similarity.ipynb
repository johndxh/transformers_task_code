{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ff1c8a9",
   "metadata": {},
   "source": [
    "# 相似性度量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c0a6d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments, DataCollatorWithPadding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4dc2addc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"json\", data_files=\"../../datas/train_pair_1w.json\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d115de7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.train_test_split(0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b95bf7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"models/macbert-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8aa65044",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_func(examples):\n",
    "    inputs = tokenizer(\n",
    "        examples[\"sentence1\"], \n",
    "        examples[\"sentence2\"],\n",
    "        max_length=128,\n",
    "        truncation=True,\n",
    "        padding=True)\n",
    "    \n",
    "    inputs[\"labels\"] = [float(label) for label in examples[\"label\"]]\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e97ebeb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2357f94864348b79ec316b6d2da45b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c4525a5697547d4960cd6ff801b1580",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_data = dataset.map(process_func, batched=True, remove_columns=dataset[\"train\"].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7bb276f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at models/macbert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\"models/macbert-base\", num_labels=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc8a523e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "combined_metrics = evaluate.combine([\"metric_accuracy.py\", \"metric_f1.py\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "67dd85ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    predictions, labels = pred\n",
    "    prediction_level = 0.5\n",
    "    predictions = [int(p > prediction_level) for p in predictions]\n",
    "    labels = [int(l) for l in labels]\n",
    "    print(len(predictions), len(labels))\n",
    "\n",
    "    return combined_metrics.compute(\n",
    "        predictions=predictions,\n",
    "        references=labels\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3ee4d38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir=\"trained/models_for_seqcrossimilarity\",\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    eval_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    logging_steps=200,\n",
    "    eval_steps=200,\n",
    "    save_steps=200,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=1,\n",
    "    optim=\"adafactor\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b616ce37",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    args=args,\n",
    "    model=model,\n",
    "    train_dataset=tokenized_data[\"train\"],\n",
    "    eval_dataset=tokenized_data[\"test\"],\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c64b0a84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [250/250 15:11, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.133800</td>\n",
       "      <td>0.088136</td>\n",
       "      <td>0.882500</td>\n",
       "      <td>0.854309</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000 2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\17521\\AppData\\Local\\Temp\\ipykernel_3776\\622599117.py:4: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  predictions = [int(p > prediction_level) for p in predictions]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=250, training_loss=0.12803606605529785, metrics={'train_runtime': 915.8551, 'train_samples_per_second': 8.735, 'train_steps_per_second': 0.273, 'total_flos': 526217385984000.0, 'train_loss': 0.12803606605529785, 'epoch': 1.0})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "31f41c04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, TextClassificationPipeline\n",
    "\n",
    "pipe = pipeline(\"text-classification\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "018ddc2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pipe({\"text\": \"我喜欢北京\", \"text_pair\": \"天气怎样\"}, function_to_apply=\"none\") # 回归任务所以用none, 不对结果做softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a706a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ce3ef541",
   "metadata": {},
   "source": [
    "# 相似性判断"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2490d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10aa41a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a259caa675654565957c5e6dffc9c389",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(\"json\", data_files=\"../../datas/train_pair_1w.json\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4fd5439d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.train_test_split(0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eacd032f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"models/macbert-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c48a698",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_func(examples):\n",
    "    sentence_list = []\n",
    "    labels = []\n",
    "    for sen1, sen2, label in zip(examples[\"sentence1\"], examples[\"sentence2\"], examples[\"label\"]):\n",
    "        sentence_list.append(sen1)\n",
    "        sentence_list.append(sen2)\n",
    "        labels.append(1 if int(label) == 1 else -1) # CosineEmbeddingLoss要求相似1, 不相似-1\n",
    "    \n",
    "    inputs = tokenizer(sentence_list, max_length=128, truncation=True, padding=True)\n",
    "    inputs = {\n",
    "        k: [v[i: i + 2] for i in range(0, len(v), 2)]\n",
    "        for k, v in inputs.items()\n",
    "    }\n",
    "    inputs[\"labels\"] = labels\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd86551a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e8ad2122f70471f87b8d20efeccac1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc80ac1a21d54835bebf5a376367b702",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_data = dataset.map(process_func, batched=True, remove_columns=dataset[\"train\"].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "116cddb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[101, 852, 3221, 8024, 2769, 2553, 7557, 6382, 3209, 100, 100, 2772, 3221, 2933, 2372, 6432, 671, 1368, 8024, 3633, 1008, 3791, 6427, 2792, 6382, 100, 100, 2769, 3315, 782, 8024, 3345, 1046, 6930, 3345, 8024, 3295, 5307, 4638, 4288, 2360, 100, 100, 5318, 2190, 3221, 702, 794, 679, 1600, 6983, 4638, 782, 8039, 2769, 1403, 3341, 3766, 1600, 100, 100, 3717, 8013, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 6432, 679, 2137, 2110, 2110, 3345, 7027, 5101, 762, 4638, 3416, 948, 3341, 2533, 5473, 3209, 8024, 800, 6432, 784, 720, 3847, 6716, 1107, 2533, 1916, 1448, 8024, 3227, 4197, 3221, 1930, 1920, 1071, 6404, 8024, 1071, 2141, 800, 1525, 7027, 3221, 1358, 749, 7599, 2170, 2798, 7410, 1358, 4638, 8024, 1921, 4495, 2218, 3221, 6821, 3416, 8024, 1600, 784, 720, 5790, 5763, 6963, 679, 5052, 752, 8024, 6206, 3221, 5473, 3209, 4157, 8024, 6820, 3221, 2515, 2419, 2110, 3345, 7027, 5101, 762, 4638, 3416, 8024, 1398, 3416, 3227, 1139, 5632, 2346, 2141, 1762, 4558, 1227, 2533, 6206, 3647, 8024, 2218, 1762, 6821, 1036, 6624, 2443, 7027, 948, 678, 1343, 8024, 6821, 671, 3341, 2218, 833, 6768, 3351, 2533, 1914, 1450, 8024, 4197, 1400, 1086, 4717, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'labels': -1}\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_data[\"train\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "74b707e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertPreTrainedModel, BertModel, BertForSequenceClassification\n",
    "from typing import Optional\n",
    "from torch.nn import CosineEmbeddingLoss, CosineSimilarity\n",
    "\n",
    "\n",
    "class BertForSimilarity(BertPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.bert = BertModel(config)\n",
    "        self.post_init()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: Optional[torch.Tensor] = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        token_type_ids: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.Tensor] = None,\n",
    "        head_mask: Optional[torch.Tensor] = None,\n",
    "        inputs_embeds: Optional[torch.Tensor] = None,\n",
    "        labels: Optional[torch.Tensor] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ):\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        A_input_ids, B_input_ids = input_ids[:, 0], input_ids[:, 1]\n",
    "        A_attention_mask, B_attention_mask = attention_mask[:, 0], attention_mask[:, 1]\n",
    "        A_token_type_ids, B_token_type_ids = token_type_ids[:, 0], token_type_ids[:, 1]\n",
    "\n",
    "        A_outputs = self.bert(\n",
    "            A_input_ids,\n",
    "            attention_mask=A_attention_mask,\n",
    "            token_type_ids=A_token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        A_pooled_output = A_outputs[1]\n",
    "\n",
    "        B_outputs = self.bert(\n",
    "            B_input_ids,\n",
    "            attention_mask=B_attention_mask,\n",
    "            token_type_ids=B_token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        B_pooled_output = B_outputs[1]\n",
    "\n",
    "        similarity = CosineSimilarity()(A_pooled_output, B_pooled_output)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_func = CosineEmbeddingLoss(0.3)\n",
    "            loss = loss_func(A_pooled_output, B_pooled_output, labels)\n",
    "        output = (similarity, )\n",
    "        return ((loss,) + output) if loss is not None else output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "61538b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForSimilarity.from_pretrained(\"models/macbert-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "171b4d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "combined_metrics = evaluate.combine([\"metric_accuracy.py\", \"metric_f1.py\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979979fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    \n",
    "    predictions, labels = pred\n",
    "    prediction_level = 0.7\n",
    "    predictions = [int(p > prediction_level) for p in predictions]\n",
    "    labels = [int(l > 0) for l in labels]\n",
    "\n",
    "    return combined_metrics.compute(\n",
    "        predictions=predictions,\n",
    "        references=labels\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "62daca59",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir=\"trained/models_for_seqAseqsimilarity\",\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    eval_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    logging_steps=200,\n",
    "    eval_steps=200,\n",
    "    save_steps=200,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=3,\n",
    "    optim=\"adafactor\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "25e29472",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "trainer = Trainer(\n",
    "    args=args,\n",
    "    model=model,\n",
    "    train_dataset=tokenized_data[\"train\"],\n",
    "    eval_dataset=tokenized_data[\"test\"],\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b032f582",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='750' max='750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [750/750 02:30, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.148500</td>\n",
       "      <td>0.196703</td>\n",
       "      <td>0.756000</td>\n",
       "      <td>0.687980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.168700</td>\n",
       "      <td>0.183045</td>\n",
       "      <td>0.783000</td>\n",
       "      <td>0.716710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.134900</td>\n",
       "      <td>0.177431</td>\n",
       "      <td>0.795000</td>\n",
       "      <td>0.731675</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000 2000\n",
      "2000 2000\n",
      "2000 2000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=750, training_loss=0.14339023081461588, metrics={'train_runtime': 151.167, 'train_samples_per_second': 158.765, 'train_steps_per_second': 4.961, 'total_flos': 3157275967488000.0, 'train_loss': 0.14339023081461588, 'epoch': 3.0})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7738199f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimilarityPipeline:\n",
    "    def __init__(self, model, tokenizer):\n",
    "        self.model = model.bert\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = model.device\n",
    "    \n",
    "    def preprocess(self, sentence1, sentence2):\n",
    "        inputs = tokenizer([sentence1, sentence2], max_length=128, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "        return inputs\n",
    "\n",
    "    def predict(self, tokens):\n",
    "        tokens = {k: v.to(self.device) for k, v in tokens.items()}\n",
    "        return self.model(**tokens)[1]\n",
    "\n",
    "    def postprocess(self, logits):\n",
    "        cos = CosineSimilarity()(logits[None, 0, :], logits[None, 1, :])\n",
    "        return cos.cpu().item()\n",
    "\n",
    "    def __call__(self, sentence1, sentence2):\n",
    "        tokens = self.preprocess(sentence1, sentence2)\n",
    "        logits = self.predict(tokens)\n",
    "        result = self.postprocess(logits)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7a68096c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = SimilarityPipeline(model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "53e8e601",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2632889747619629"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe(\"我喜欢北京\", \"明天不行\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d4d0e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
