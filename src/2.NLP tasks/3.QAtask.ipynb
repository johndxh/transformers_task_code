{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "361e1f97",
   "metadata": {},
   "source": [
    "# QA问题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6a62438f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from datasets import DatasetDict\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, Trainer, TrainingArguments, DefaultDataCollator, BertForQuestionAnswering, BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c483bac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DatasetDict.load_from_disk(\"../../datas/mrc_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3f8b9207",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'TRAIN_186_QUERY_0',\n",
       " 'context': '范廷颂枢机（，），圣名保禄·若瑟（），是越南罗马天主教枢机。1963年被任为主教；1990年被擢升为天主教河内总教区宗座署理；1994年被擢升为总主教，同年年底被擢升为枢机；2009年2月离世。范廷颂于1919年6月15日在越南宁平省天主教发艳教区出生；童年时接受良好教育后，被一位越南神父带到河内继续其学业。范廷颂于1940年在河内大修道院完成神学学业。范廷颂于1949年6月6日在河内的主教座堂晋铎；及后被派到圣女小德兰孤儿院服务。1950年代，范廷颂在河内堂区创建移民接待中心以收容到河内避战的难民。1954年，法越战争结束，越南民主共和国建都河内，当时很多天主教神职人员逃至越南的南方，但范廷颂仍然留在河内。翌年管理圣若望小修院；惟在1960年因捍卫修院的自由、自治及拒绝政府在修院设政治课的要求而被捕。1963年4月5日，教宗任命范廷颂为天主教北宁教区主教，同年8月15日就任；其牧铭为「我信天主的爱」。由于范廷颂被越南政府软禁差不多30年，因此他无法到所属堂区进行牧灵工作而专注研读等工作。范廷颂除了面对战争、贫困、被当局迫害天主教会等问题外，也秘密恢复修院、创建女修会团体等。1990年，教宗若望保禄二世在同年6月18日擢升范廷颂为天主教河内总教区宗座署理以填补该教区总主教的空缺。1994年3月23日，范廷颂被教宗若望保禄二世擢升为天主教河内总教区总主教并兼天主教谅山教区宗座署理；同年11月26日，若望保禄二世擢升范廷颂为枢机。范廷颂在1995年至2001年期间出任天主教越南主教团主席。2003年4月26日，教宗若望保禄二世任命天主教谅山教区兼天主教高平教区吴光杰主教为天主教河内总教区署理主教；及至2005年2月19日，范廷颂因获批辞去总主教职务而荣休；吴光杰同日真除天主教河内总教区总主教职务。范廷颂于2009年2月22日清晨在河内离世，享年89岁；其葬礼于同月26日上午在天主教河内总教区总主教座堂举行。',\n",
       " 'question': '范廷颂是什么时候被任为主教的？',\n",
       " 'answers': {'text': ['1963年'], 'answer_start': [30]}}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "78922fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"models/macbert-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "dbf1cb69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_func(examples):\n",
    "    inputs = tokenizer(\n",
    "        examples[\"question\"],\n",
    "        examples[\"context\"],\n",
    "        max_length=512,\n",
    "        truncation=\"only_second\",\n",
    "        padding=True,\n",
    "        return_offsets_mapping=True\n",
    "    )\n",
    "\n",
    "    offsets_mapping = inputs.pop(\"offset_mapping\")\n",
    "    start_postitions = []\n",
    "    end_postitions = []\n",
    "\n",
    "    for idx, offset_mapping in enumerate(offsets_mapping):\n",
    "        # 定位原句子中答案的位置\n",
    "        answer = examples[\"answers\"][idx]\n",
    "        answer_start_pos = answer[\"answer_start\"][0]\n",
    "        answer_end_pos = answer_start_pos + len(answer[\"text\"][0])\n",
    "        \n",
    "        # 将tokenize后的QA对的答案定位到A的开始位置和结束位置\n",
    "        context_start = inputs.sequence_ids(batch_index=idx).index(1)\n",
    "        context_end = inputs.sequence_ids(batch_index=idx).index(None, context_start) - 1\n",
    "\n",
    "        # QA对不包含原句子的答案\n",
    "        if offset_mapping[context_end][1] < answer_start_pos or \\\n",
    "            offset_mapping[context_start][0] > answer_end_pos:\n",
    "            start_pos = 0\n",
    "            end_pos = 0\n",
    "        \n",
    "        # QA对包含原句子的答案\n",
    "        else:\n",
    "            # 左指针向原答案开始位置逼近\n",
    "            tokenid = context_start\n",
    "            while offset_mapping[tokenid][0] < answer_start_pos and tokenid <= context_end:\n",
    "                tokenid += 1\n",
    "            context_start = start_pos = tokenid\n",
    "\n",
    "            # 右指针向原答案结束位置逼近\n",
    "            tokenid = context_end\n",
    "            while offset_mapping[tokenid][1] > answer_end_pos and tokenid >= context_start:\n",
    "                tokenid -= 1\n",
    "            context_end = end_pos = tokenid\n",
    "        \n",
    "        start_postitions.append(start_pos)\n",
    "        end_postitions.append(end_pos)\n",
    "    \n",
    "    inputs[\"start_positions\"] = start_postitions\n",
    "    inputs[\"end_positions\"] = end_postitions\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4f8ad2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_data = dataset.map(process_func, batched=True, remove_columns=dataset[\"train\"].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "047d20dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'start_positions', 'end_positions'],\n",
       "        num_rows: 10142\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'start_positions', 'end_positions'],\n",
       "        num_rows: 3219\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'start_positions', 'end_positions'],\n",
       "        num_rows: 1002\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d7da948e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at models/macbert-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForQuestionAnswering.from_pretrained(\"models/macbert-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6ee2180e",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir=\"./trained/model_for_qa\",\n",
    "    per_device_eval_batch_size=32,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=8,\n",
    "    gradient_checkpointing=True,\n",
    "    save_steps=200,\n",
    "    eval_steps=200,\n",
    "    logging_steps=200,\n",
    "    optim=\"adafactor\",\n",
    "    num_train_epochs=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "df900983",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    args=args,\n",
    "    model=model,\n",
    "    train_dataset=tokenized_data[\"train\"],\n",
    "    eval_dataset=tokenized_data[\"validation\"],\n",
    "    data_collator=DefaultDataCollator()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "fd1e2062",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='317' max='317' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [317/317 02:57, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.962300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=317, training_loss=1.7391600984880224, metrics={'train_runtime': 178.3355, 'train_samples_per_second': 56.87, 'train_steps_per_second': 1.778, 'total_flos': 2650071706816512.0, 'train_loss': 1.7391600984880224, 'epoch': 1.0})"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "31dcceca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9a3e8d8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(\"question-answering\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "69821a31",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.12/site-packages/torch/utils/checkpoint.py:86: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'score': 0.31867194175720215, 'start': 3, 'end': 5, 'answer': '北京'}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe(question=\"小明在哪里上班？\", context=\"小明在北京上班。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e459a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5b79e1ff",
   "metadata": {},
   "source": [
    "# 滑动窗口版本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9935832a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from datasets import DatasetDict\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, Trainer, TrainingArguments, DefaultDataCollator, BertForQuestionAnswering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ca1e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DatasetDict.load_from_disk(\"../../datas/mrc_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf5a67f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"models/macbert-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15446ba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 2769, 102, 2571, 727, 102], [101, 3221, 102, 2571, 727, 102], [101, 3221, 102, 4638, 3215, 102], [101, 3221, 102, 4344, 102], [101, 671, 102, 2571, 727, 102], [101, 671, 102, 4638, 3215, 102], [101, 671, 102, 4344, 102], [101, 1372, 102, 2571, 727, 102], [101, 1372, 102, 4638, 3215, 102], [101, 1372, 102, 4344, 102], [101, 4344, 102, 2571, 727, 102], [101, 4344, 102, 4638, 3215, 102], [101, 4344, 102, 4344, 102], [101, 2769, 102, 4638, 3215, 102], [101, 2769, 102, 4344, 102], [101, 2769, 3221, 102, 7029, 102], [101, 1504, 1568, 102, 7029, 102], [101, 1504, 1568, 102, 3683, 102], [101, 1504, 1568, 102, 1920, 102], [101, 1504, 1568, 102, 7413, 102], [101, 143, 3457, 102, 7029, 102], [101, 143, 3457, 102, 3683, 102], [101, 143, 3457, 102, 1920, 102], [101, 143, 3457, 102, 7413, 102], [101, 2769, 3221, 102, 3683, 102], [101, 2769, 3221, 102, 1920, 102], [101, 2769, 3221, 102, 7413, 102], [101, 2769, 102, 2769, 6206, 102], [101, 3221, 102, 2769, 6206, 102], [101, 3221, 102, 2828, 2769, 102], [101, 3221, 102, 4017, 2791, 102], [101, 3221, 102, 2094, 102], [101, 671, 102, 2769, 6206, 102], [101, 671, 102, 2828, 2769, 102], [101, 671, 102, 4017, 2791, 102], [101, 671, 102, 2094, 102], [101, 702, 102, 2769, 6206, 102], [101, 702, 102, 2828, 2769, 102], [101, 702, 102, 4017, 2791, 102], [101, 702, 102, 2094, 102], [101, 5106, 102, 2769, 6206, 102], [101, 5106, 102, 2828, 2769, 102], [101, 5106, 102, 4017, 2791, 102], [101, 5106, 102, 2094, 102], [101, 1170, 102, 2769, 6206, 102], [101, 1170, 102, 2828, 2769, 102], [101, 1170, 102, 4017, 2791, 102], [101, 1170, 102, 2094, 102], [101, 1269, 102, 2769, 6206, 102], [101, 1269, 102, 2828, 2769, 102], [101, 1269, 102, 4017, 2791, 102], [101, 1269, 102, 2094, 102], [101, 2769, 102, 2828, 2769, 102], [101, 2769, 102, 4017, 2791, 102], [101, 2769, 102, 2094, 102]], 'token_type_ids': [[0, 0, 0, 1, 1, 1], [0, 0, 0, 1, 1, 1], [0, 0, 0, 1, 1, 1], [0, 0, 0, 1, 1], [0, 0, 0, 1, 1, 1], [0, 0, 0, 1, 1, 1], [0, 0, 0, 1, 1], [0, 0, 0, 1, 1, 1], [0, 0, 0, 1, 1, 1], [0, 0, 0, 1, 1], [0, 0, 0, 1, 1, 1], [0, 0, 0, 1, 1, 1], [0, 0, 0, 1, 1], [0, 0, 0, 1, 1, 1], [0, 0, 0, 1, 1], [0, 0, 0, 0, 1, 1], [0, 0, 0, 0, 1, 1], [0, 0, 0, 0, 1, 1], [0, 0, 0, 0, 1, 1], [0, 0, 0, 0, 1, 1], [0, 0, 0, 0, 1, 1], [0, 0, 0, 0, 1, 1], [0, 0, 0, 0, 1, 1], [0, 0, 0, 0, 1, 1], [0, 0, 0, 0, 1, 1], [0, 0, 0, 0, 1, 1], [0, 0, 0, 0, 1, 1], [0, 0, 0, 1, 1, 1], [0, 0, 0, 1, 1, 1], [0, 0, 0, 1, 1, 1], [0, 0, 0, 1, 1, 1], [0, 0, 0, 1, 1], [0, 0, 0, 1, 1, 1], [0, 0, 0, 1, 1, 1], [0, 0, 0, 1, 1, 1], [0, 0, 0, 1, 1], [0, 0, 0, 1, 1, 1], [0, 0, 0, 1, 1, 1], [0, 0, 0, 1, 1, 1], [0, 0, 0, 1, 1], [0, 0, 0, 1, 1, 1], [0, 0, 0, 1, 1, 1], [0, 0, 0, 1, 1, 1], [0, 0, 0, 1, 1], [0, 0, 0, 1, 1, 1], [0, 0, 0, 1, 1, 1], [0, 0, 0, 1, 1, 1], [0, 0, 0, 1, 1], [0, 0, 0, 1, 1, 1], [0, 0, 0, 1, 1, 1], [0, 0, 0, 1, 1, 1], [0, 0, 0, 1, 1], [0, 0, 0, 1, 1, 1], [0, 0, 0, 1, 1, 1], [0, 0, 0, 1, 1]], 'attention_mask': [[1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1]], 'overflow_to_sample_mapping': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = tokenizer([\"我是一只猫\", \"我是哆啦A梦\", \"我是一个粉刷匠\"], [\"快乐的星猫\", \"野比大雄\", \"我要把我滴房子\"], return_overflowing_tokens=1, max_length=6)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad13138d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'TRAIN_186_QUERY_0',\n",
       " 'context': '范廷颂枢机（，），圣名保禄·若瑟（），是越南罗马天主教枢机。1963年被任为主教；1990年被擢升为天主教河内总教区宗座署理；1994年被擢升为总主教，同年年底被擢升为枢机；2009年2月离世。范廷颂于1919年6月15日在越南宁平省天主教发艳教区出生；童年时接受良好教育后，被一位越南神父带到河内继续其学业。范廷颂于1940年在河内大修道院完成神学学业。范廷颂于1949年6月6日在河内的主教座堂晋铎；及后被派到圣女小德兰孤儿院服务。1950年代，范廷颂在河内堂区创建移民接待中心以收容到河内避战的难民。1954年，法越战争结束，越南民主共和国建都河内，当时很多天主教神职人员逃至越南的南方，但范廷颂仍然留在河内。翌年管理圣若望小修院；惟在1960年因捍卫修院的自由、自治及拒绝政府在修院设政治课的要求而被捕。1963年4月5日，教宗任命范廷颂为天主教北宁教区主教，同年8月15日就任；其牧铭为「我信天主的爱」。由于范廷颂被越南政府软禁差不多30年，因此他无法到所属堂区进行牧灵工作而专注研读等工作。范廷颂除了面对战争、贫困、被当局迫害天主教会等问题外，也秘密恢复修院、创建女修会团体等。1990年，教宗若望保禄二世在同年6月18日擢升范廷颂为天主教河内总教区宗座署理以填补该教区总主教的空缺。1994年3月23日，范廷颂被教宗若望保禄二世擢升为天主教河内总教区总主教并兼天主教谅山教区宗座署理；同年11月26日，若望保禄二世擢升范廷颂为枢机。范廷颂在1995年至2001年期间出任天主教越南主教团主席。2003年4月26日，教宗若望保禄二世任命天主教谅山教区兼天主教高平教区吴光杰主教为天主教河内总教区署理主教；及至2005年2月19日，范廷颂因获批辞去总主教职务而荣休；吴光杰同日真除天主教河内总教区总主教职务。范廷颂于2009年2月22日清晨在河内离世，享年89岁；其葬礼于同月26日上午在天主教河内总教区总主教座堂举行。',\n",
       " 'question': '范廷颂是什么时候被任为主教的？',\n",
       " 'answers': {'text': ['1963年'], 'answer_start': [30]}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f9e9b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_func(examples):\n",
    "    inputs = tokenizer(\n",
    "        examples[\"question\"],\n",
    "        examples[\"context\"],\n",
    "        max_length=384,\n",
    "        truncation=\"only_second\",\n",
    "        padding=True,\n",
    "        return_offsets_mapping=True,\n",
    "        return_overflowing_tokens=True,\n",
    "        stride=128\n",
    "    )\n",
    "\n",
    "    overflow_tokens = inputs.pop(\"overflow_to_sample_mapping\")\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "    example_ids = []\n",
    "\n",
    "    for idx, sentence_id in enumerate(overflow_tokens):\n",
    "        answer = examples[\"answers\"][sentence_id]\n",
    "        answer_start_pos = answer[\"answer_start\"][0]\n",
    "        answer_end_pos = answer_start_pos + len(answer[\"text\"][0])\n",
    "    \n",
    "        context_start = inputs.sequence_ids(idx).index(1)\n",
    "        context_end = inputs.sequence_ids(idx).index(None, context_start) - 1\n",
    "        offset_mapping = inputs[\"offset_mapping\"][idx]\n",
    "\n",
    "        if offset_mapping[context_end][1] < answer_start_pos or offset_mapping[context_start][0] > answer_end_pos:\n",
    "            start_pos = 0\n",
    "            end_pos = 0\n",
    "        \n",
    "        else:\n",
    "            tokenid = context_start\n",
    "            while offset_mapping[tokenid][0] < answer_start_pos and tokenid <= context_end:\n",
    "                tokenid += 1\n",
    "            context_start = start_pos = tokenid\n",
    "\n",
    "            tokenid = context_end\n",
    "            while offset_mapping[tokenid][1] > answer_end_pos and tokenid >= context_start:\n",
    "                tokenid -= 1\n",
    "            context_end = end_pos = tokenid\n",
    "\n",
    "        start_positions.append(start_pos)\n",
    "        end_positions.append(end_pos)\n",
    "        example_ids.append(examples[\"id\"][sentence_id]) \n",
    "        inputs[\"offset_mapping\"][idx] = [\n",
    "            (v if inputs.sequence_ids(idx)[k] == 1 else None)\n",
    "            for k, v in enumerate(inputs[\"offset_mapping\"][idx])\n",
    "        ]\n",
    "\n",
    "    inputs[\"start_positions\"] = start_positions\n",
    "    inputs[\"end_positions\"] = end_positions\n",
    "    inputs[\"example_ids\"] = example_ids\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20ced2fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75831cd862074432804a37a9d1b35bca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1002 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_data = dataset.map(process_func, batched=True, remove_columns=dataset[\"train\"].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e275fdf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 5745, 2455, 7563, 3221, 784, 720, 3198, 952, 6158, 818, 711, 712, 3136, 4638, 8043, 102, 5745, 2455, 7563, 3364, 3322, 8020, 8024, 8021, 8024, 1760, 1399, 924, 4882, 185, 5735, 4449, 8020, 8021, 8024, 3221, 6632, 1298, 5384, 7716, 1921, 712, 3136, 3364, 3322, 511, 9155, 2399, 6158, 818, 711, 712, 3136, 8039, 8431, 2399, 6158, 3091, 1285, 711, 1921, 712, 3136, 3777, 1079, 2600, 3136, 1277, 2134, 2429, 5392, 4415, 8039, 8447, 2399, 6158, 3091, 1285, 711, 2600, 712, 3136, 8024, 1398, 2399, 2399, 2419, 6158, 3091, 1285, 711, 3364, 3322, 8039, 8170, 2399, 123, 3299, 4895, 686, 511, 5745, 2455, 7563, 754, 9915, 2399, 127, 3299, 8115, 3189, 1762, 6632, 1298, 2123, 2398, 4689, 1921, 712, 3136, 1355, 5683, 3136, 1277, 1139, 4495, 8039, 4997, 2399, 3198, 2970, 1358, 5679, 1962, 3136, 5509, 1400, 8024, 6158, 671, 855, 6632, 1298, 4868, 4266, 2372, 1168, 3777, 1079, 5326, 5330, 1071, 2110, 689, 511, 5745, 2455, 7563, 754, 9211, 2399, 1762, 3777, 1079, 1920, 934, 6887, 7368, 2130, 2768, 4868, 2110, 2110, 689, 511, 5745, 2455, 7563, 754, 8594, 2399, 127, 3299, 127, 3189, 1762, 3777, 1079, 4638, 712, 3136, 2429, 1828, 3232, 7195, 8039, 1350, 1400, 6158, 3836, 1168, 1760, 1957, 2207, 2548, 1065, 2109, 1036, 7368, 3302, 1218, 511, 8707, 2399, 807, 8024, 5745, 2455, 7563, 1762, 3777, 1079, 1828, 1277, 1158, 2456, 4919, 3696, 2970, 2521, 704, 2552, 809, 3119, 2159, 1168, 3777, 1079, 6912, 2773, 4638, 7410, 3696, 511, 9258, 2399, 8024, 3791, 6632, 2773, 751, 5310, 3338, 8024, 6632, 1298, 3696, 712, 1066, 1469, 1744, 2456, 6963, 3777, 1079, 8024, 2496, 3198, 2523, 1914, 1921, 712, 3136, 4868, 5466, 782, 1447, 6845, 5635, 6632, 1298, 4638, 1298, 3175, 8024, 852, 5745, 2455, 7563, 793, 4197, 4522, 1762, 3777, 1079, 511, 5422, 2399, 5052, 4415, 1760, 5735, 3307, 2207, 934, 7368, 8039, 2668, 1762, 8779, 2399, 1728, 2932, 1310, 934, 7368, 4638, 5632, 4507, 510, 5632, 3780, 1350, 2867, 5318, 3124, 2424, 1762, 934, 7368, 6392, 3124, 3780, 6440, 4638, 6206, 3724, 5445, 6158, 2936, 511, 9155, 2399, 125, 3299, 126, 3189, 8024, 3136, 2134, 818, 1462, 5745, 2455, 7563, 711, 1921, 712, 3136, 1266, 2123, 3136, 1277, 712, 3136, 8024, 1398, 2399, 129, 3299, 8115, 3189, 2218, 818, 8039, 1071, 4288, 7208, 711, 519, 2769, 928, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'offset_mapping': [None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, [0, 1], [1, 2], [2, 3], [3, 4], [4, 5], [5, 6], [6, 7], [7, 8], [8, 9], [9, 10], [10, 11], [11, 12], [12, 13], [13, 14], [14, 15], [15, 16], [16, 17], [17, 18], [18, 19], [19, 20], [20, 21], [21, 22], [22, 23], [23, 24], [24, 25], [25, 26], [26, 27], [27, 28], [28, 29], [29, 30], [30, 34], [34, 35], [35, 36], [36, 37], [37, 38], [38, 39], [39, 40], [40, 41], [41, 45], [45, 46], [46, 47], [47, 48], [48, 49], [49, 50], [50, 51], [51, 52], [52, 53], [53, 54], [54, 55], [55, 56], [56, 57], [57, 58], [58, 59], [59, 60], [60, 61], [61, 62], [62, 63], [63, 67], [67, 68], [68, 69], [69, 70], [70, 71], [71, 72], [72, 73], [73, 74], [74, 75], [75, 76], [76, 77], [77, 78], [78, 79], [79, 80], [80, 81], [81, 82], [82, 83], [83, 84], [84, 85], [85, 86], [86, 87], [87, 91], [91, 92], [92, 93], [93, 94], [94, 95], [95, 96], [96, 97], [97, 98], [98, 99], [99, 100], [100, 101], [101, 105], [105, 106], [106, 107], [107, 108], [108, 110], [110, 111], [111, 112], [112, 113], [113, 114], [114, 115], [115, 116], [116, 117], [117, 118], [118, 119], [119, 120], [120, 121], [121, 122], [122, 123], [123, 124], [124, 125], [125, 126], [126, 127], [127, 128], [128, 129], [129, 130], [130, 131], [131, 132], [132, 133], [133, 134], [134, 135], [135, 136], [136, 137], [137, 138], [138, 139], [139, 140], [140, 141], [141, 142], [142, 143], [143, 144], [144, 145], [145, 146], [146, 147], [147, 148], [148, 149], [149, 150], [150, 151], [151, 152], [152, 153], [153, 154], [154, 155], [155, 156], [156, 157], [157, 158], [158, 159], [159, 163], [163, 164], [164, 165], [165, 166], [166, 167], [167, 168], [168, 169], [169, 170], [170, 171], [171, 172], [172, 173], [173, 174], [174, 175], [175, 176], [176, 177], [177, 178], [178, 179], [179, 180], [180, 181], [181, 182], [182, 186], [186, 187], [187, 188], [188, 189], [189, 190], [190, 191], [191, 192], [192, 193], [193, 194], [194, 195], [195, 196], [196, 197], [197, 198], [198, 199], [199, 200], [200, 201], [201, 202], [202, 203], [203, 204], [204, 205], [205, 206], [206, 207], [207, 208], [208, 209], [209, 210], [210, 211], [211, 212], [212, 213], [213, 214], [214, 215], [215, 216], [216, 217], [217, 218], [218, 222], [222, 223], [223, 224], [224, 225], [225, 226], [226, 227], [227, 228], [228, 229], [229, 230], [230, 231], [231, 232], [232, 233], [233, 234], [234, 235], [235, 236], [236, 237], [237, 238], [238, 239], [239, 240], [240, 241], [241, 242], [242, 243], [243, 244], [244, 245], [245, 246], [246, 247], [247, 248], [248, 249], [249, 250], [250, 251], [251, 252], [252, 253], [253, 257], [257, 258], [258, 259], [259, 260], [260, 261], [261, 262], [262, 263], [263, 264], [264, 265], [265, 266], [266, 267], [267, 268], [268, 269], [269, 270], [270, 271], [271, 272], [272, 273], [273, 274], [274, 275], [275, 276], [276, 277], [277, 278], [278, 279], [279, 280], [280, 281], [281, 282], [282, 283], [283, 284], [284, 285], [285, 286], [286, 287], [287, 288], [288, 289], [289, 290], [290, 291], [291, 292], [292, 293], [293, 294], [294, 295], [295, 296], [296, 297], [297, 298], [298, 299], [299, 300], [300, 301], [301, 302], [302, 303], [303, 304], [304, 305], [305, 306], [306, 307], [307, 308], [308, 309], [309, 310], [310, 311], [311, 312], [312, 313], [313, 314], [314, 315], [315, 316], [316, 317], [317, 318], [318, 319], [319, 320], [320, 321], [321, 325], [325, 326], [326, 327], [327, 328], [328, 329], [329, 330], [330, 331], [331, 332], [332, 333], [333, 334], [334, 335], [335, 336], [336, 337], [337, 338], [338, 339], [339, 340], [340, 341], [341, 342], [342, 343], [343, 344], [344, 345], [345, 346], [346, 347], [347, 348], [348, 349], [349, 350], [350, 351], [351, 352], [352, 353], [353, 354], [354, 355], [355, 356], [356, 360], [360, 361], [361, 362], [362, 363], [363, 364], [364, 365], [365, 366], [366, 367], [367, 368], [368, 369], [369, 370], [370, 371], [371, 372], [372, 373], [373, 374], [374, 375], [375, 376], [376, 377], [377, 378], [378, 379], [379, 380], [380, 381], [381, 382], [382, 383], [383, 384], [384, 385], [385, 386], [386, 387], [387, 388], [388, 390], [390, 391], [391, 392], [392, 393], [393, 394], [394, 395], [395, 396], [396, 397], [397, 398], [398, 399], [399, 400], [400, 401], None], 'start_positions': 47, 'end_positions': 48, 'example_ids': 'TRAIN_186_QUERY_0'}\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_data[\"train\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "af4e8e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections # 聚合答案\n",
    "\n",
    "def gather_result(start_logits, end_logits, dataset, tokenized_data):\n",
    "    predictions = {}\n",
    "    references = {}\n",
    "\n",
    "    example_to_tokens = collections.defaultdict(list)\n",
    "    for idx, example_id in enumerate(tokenized_data[\"example_ids\"]):\n",
    "        example_to_tokens[example_id].append(idx)\n",
    "\n",
    "    n_best = 20\n",
    "    max_ans_length = 30\n",
    "\n",
    "    for data in dataset:\n",
    "        example_id = data[\"id\"]\n",
    "        answers = []\n",
    "\n",
    "        for idx in example_to_tokens[example_id]:\n",
    "            offset_mapping = tokenized_data[idx][\"offset_mapping\"]\n",
    "            token_start_logits = start_logits[idx].argsort()[::-1][:n_best].tolist()\n",
    "            token_end_logits = end_logits[idx].argsort()[::-1][:n_best].tolist()\n",
    "\n",
    "            for start_pos in token_start_logits:\n",
    "                for end_pos in token_end_logits:\n",
    "                    if (end_pos < start_pos) or (end_pos - start_pos + 1 > max_ans_length):\n",
    "                        continue\n",
    "                    if offset_mapping[start_pos] is None or offset_mapping[end_pos] is None:\n",
    "                        continue\n",
    "                    answers.append({\n",
    "                        \"text\": data[\"context\"][offset_mapping[start_pos][0]: offset_mapping[end_pos][1]],\n",
    "                        \"score\": start_logits[idx][start_pos] + end_logits[idx][end_pos]\n",
    "                    })\n",
    "            \n",
    "            if len(answers) > 0:\n",
    "                best_answer = max(answers, key=lambda x: x[\"score\"])\n",
    "                predictions[example_id] = best_answer[\"text\"]\n",
    "            else:\n",
    "                predictions[example_id] = \"\"\n",
    "            references[example_id] = data[\"answers\"][\"text\"]\n",
    "\n",
    "    return predictions, references\n",
    "\n",
    "# trainer.evaluate(tokenized_data[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c9d795ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1545.27s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://mirrors.aliyun.com/pypi/simple\n",
      "Requirement already satisfied: nltk in /root/miniconda3/lib/python3.12/site-packages (3.9.1)\n",
      "Requirement already satisfied: click in /root/miniconda3/lib/python3.12/site-packages (from nltk) (8.2.1)\n",
      "Requirement already satisfied: joblib in /root/miniconda3/lib/python3.12/site-packages (from nltk) (1.5.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /root/miniconda3/lib/python3.12/site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /root/miniconda3/lib/python3.12/site-packages (from nltk) (4.67.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a5677db3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b988128a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cmrc_eval import evaluate_cmrc\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    start_logits, end_logits = pred[0]\n",
    "\n",
    "    label = \"validation\" if start_logits.shape[0] == len(tokenized_data[\"validation\"]) else \"test\"\n",
    "    predictions, labels = gather_result(start_logits, end_logits, dataset[label], tokenized_data[label])\n",
    "\n",
    "    return evaluate_cmrc(\n",
    "        predictions=predictions,\n",
    "        references=labels\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "57e3d4d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at models/macbert-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForQuestionAnswering.from_pretrained(\"models/macbert-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ef243228",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    per_device_eval_batch_size=32,\n",
    "    per_device_train_batch_size=32,\n",
    "    output_dir=\"./trained/model_for_qa_slide\",\n",
    "    gradient_checkpointing=True,\n",
    "    optim=\"adafactor\",\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=200,\n",
    "    save_steps=200,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=1,\n",
    "    logging_steps=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b2c56d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    args=args,\n",
    "    model=model,\n",
    "    train_dataset=tokenized_data[\"train\"],\n",
    "    eval_dataset=tokenized_data[\"validation\"],\n",
    "    data_collator=DefaultDataCollator(),\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5c3fafb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='600' max='600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [600/600 05:15, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Avg</th>\n",
       "      <th>F1</th>\n",
       "      <th>Em</th>\n",
       "      <th>Total</th>\n",
       "      <th>Skip</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.578400</td>\n",
       "      <td>1.279770</td>\n",
       "      <td>75.816359</td>\n",
       "      <td>85.028182</td>\n",
       "      <td>66.604536</td>\n",
       "      <td>3219</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.399200</td>\n",
       "      <td>1.174158</td>\n",
       "      <td>78.587818</td>\n",
       "      <td>87.402414</td>\n",
       "      <td>69.773221</td>\n",
       "      <td>3219</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.316300</td>\n",
       "      <td>1.167351</td>\n",
       "      <td>77.824613</td>\n",
       "      <td>87.398216</td>\n",
       "      <td>68.251010</td>\n",
       "      <td>3219</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=600, training_loss=1.5798635609944662, metrics={'train_runtime': 316.0931, 'train_samples_per_second': 60.707, 'train_steps_per_second': 1.898, 'total_flos': 3760517598755328.0, 'train_loss': 1.5798635609944662, 'epoch': 1.0})"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a60bd802",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [63/63 00:06]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 1.415410041809082,\n",
       " 'eval_avg': 51.84703984041357,\n",
       " 'eval_f1': 69.46254275467945,\n",
       " 'eval_em': 34.231536926147704,\n",
       " 'eval_total': 1002,\n",
       " 'eval_skip': 0,\n",
       " 'eval_runtime': 10.1608,\n",
       " 'eval_samples_per_second': 195.653,\n",
       " 'eval_steps_per_second': 6.2,\n",
       " 'epoch': 1.0}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(tokenized_data[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fb89560a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c07ba677",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(\"question-answering\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4700b906",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.4139211177825928, 'start': 3, 'end': 5, 'answer': '北京'}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe(question=\"小明在哪里上班？\", context=\"小明在北京上班。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c808d0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87653382",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
